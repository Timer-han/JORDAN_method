Рассмотрим сообщения - основной объект с корорым работает MPI.
Сообщение состоит:
1) Блок данных типа `void*`
2) Метаданные - информация по данных
	- Типа данных MPI_Datatype
	- Количество данных этого типа
		little endian (arm, Power PC, sun) or
		big endian (intel)
		Поэтому очень важно знание архитектуры процессора, что и передаётся
3) Информация о получателе и отправителе
	- Идентификатор группы = коммутатор
		MPI_COMM_WORLD
	- Идентификатор получателя (ранг)
		Отсутствует при коллективном обмене, стоит маркер "все"
4) Тег сообщения (отсутствует в коллективном обмене)
	Выполняет роль фильтра, число от $0$ до $2^{15} - 1$
	Обычно не используется, в функциях попарного обмена просто ставим 0
	Получать можно с тегом, но сбросить его можно с MPI_ANY_TAG

### MPI_Datatype
Для стандартных типов языка Си уже построено соответствие, но можно создавать и свои

| MPI_Datatype       | C type               |
| ------------------ | -------------------- |
| MPI_CHAR           | char                 | 
| MPI_INT            | int                  |
| MPI_SHORT          | short int            |
| MPI_LONG           | long int             |
| MPI_UNSIGNED_CHAR  | unsigned char        |
| MPI_UNSIGNED_SHORT | unsigned short       |
| MPI_UNSIGNED_INT   | unsigned int         |
| MPI_UNSIGNED_LONG  | unsigned long        |
| MPI_FLOAT          | float                |
| MPI_DOUBLE         | double               |
| MPI_LONG_DOUBLE    | long double          |
| MPI_BYTE           | unsigned char        |
| MPI_PACKED         | Для своих типов      |
| MPI_Comm           | Идентификатор группы |
| MPI_Group          | Сама группа          |

MPI_COMM_WORLD
Подгруппы:
- Учёт топологии
- Библиотеки

Получить число процессов в группе с идентификатором comm
```cpp
int MPI_Comm_size(MPI_Comm comm, int *size);
```

Номер процесса в группе
```cpp
int MPI_Comm_rank(MPI_Comm comm, int *rank);
```

```shell
mpirun -np 24 ./a.out
# Номера от 0 до 23
```

```cpp
#include "mpi.h"

int main(int argc, char *argv[])
{
	int p, k;
	MPI_Init(&argc, &argv);
	MPI_Comm_size(MPI_COMM_WORLD, &p); // 24
	MPI_Comm_rank(MPI_COMM_WORLD, &k); // 0...23

	solve(MPI_COMM_WORLD, p, ...);
	MPI_Finalize();
	return 0;
}
```

Теперь можем сформировать нашу матрицу и делить её между процессорами.

```cpp
int solve(MPI_Comm, int p, int n)
{
	
}
```

```cpp
int MPI_Send(
	void *buf, // что отправляем
	int countm, // сколько
	MPI_Datatype type, // какого типа
	int dest, // кому
	int tag, // тэг
	MPI_Comm comm // идентификатор группы
);
int MPI_Recv(
	void *buf, // что получаем
	int countm, // сколько
	MPI_Datatype type, // какого типа
	int src, // от кого
	int tag, // с каким тегом должны получить
	MPI_Comm comm, // идентификатор группы
	MPI_Status *status // сколько реально данных получено
			    	   // можно получить тэг (MPI_ANY_TAG)
			    	   // src (если MPI_ANY_SOURCE)
);
```

MPI_Status - структура, которая зависит от реализации. Доступ к ней через функции (FICE*)
Например:
```cpp
int MPI_Get_count(
	MPI_Status *status,
	MPI_Datatype type,
	int *count
);
```
![[IMG_8877.jpg]]
![[IMG_8881.jpg]]
stdin вообще не существует и будет seg pipe
stdout интереснее:
Процесс с номером 0 главный - запустил job manager
Остлальные появились MPI_Init
stdout главного процесса копируется в файл, который будет доступен по завершению процесса.  
При локальном запуске вывод один.

### Чтение из файлов
1) Все читают и все оставляют часть
2) Один читает и отдаёд другим "их" часть
   
| Процесс | Строка |
| ------- | ------ |
| 0       |        | 
| 1       |        |
| 2       |        |
| 0       |        |
| 1       |        |
| 2       |        |
| 0       |        |
| 1       |        |

Один процесс читает построчно и отправляет данные нужному процессу
### Запись в файл
ПИШЕТ ТОЛЬКО ОДИН, иначе последний затерёт
Для отладки каждый процесс пишет в __свой__ файл

## Hello world в MPI
```cpp
#include "mpi.h"
#include <stdio.h>

#define LEN 1234 // длина буфера

int main(int argc, char *argv[])
{
	int p, n;
	MPI_Comm comm = MPI_COMM_WORLD;
	int tag = 0;
	char buf[LEN];
	MPI_Status status;
	MPI_Iint(&argc, &argv);
	MPI_Comm_size(comm, &p);
	MPI_Comm_rank(comm, &n);
	
	snprintf(buf, LEN, "Hello from process %d!\n", k);

	if (k != 0) {
		// не главный процесс отправляет строку в главный
		MPI_Send(
			buf,
			strlen(buf) + 1,
			MPI_CHAR,
			0, // destination
			tag,
			comm
		);
	} else { // главный
		// Hello от себя родить
		printf("%s\n", buf);
		for (int i = 1; i < p; i++) {
			MPI_Recv(
				buf,
				LEN, 
				MPI_CHAR, //type
				i, //source. Можно указать MPI_ANY_SOURCE, тогда будет случайно
				tag,
				comm,
				&status
			);
			printf("%s", buf);
		}
	}
	MPI_Finalize();
	return 0;
}
```

## Как написать печать матрицы?
![[IMG_8883.jpg]]
Циклический сдвиг!
![[IMG_8884.jpg]]
Но если блок слишком большой, то будет DEADLOCK, потому что все процессы будут разом ждать ответ.
Но выход придумали, написали одновременный send и receive!

```cpp
int MPI_Sendrecv(
	void *sendbuf, // что отправляем
	int sendcount, // сколько
	MPI_Datatype sendtype, // какого типа
	int dest, // кому
	int sendtag, // тэг отправляемого
	void *recvbuf, // что получаем
	int recvcount, // сколько
	MPI_Datatype recvtype, // какого типа
	int src, // от кого
	int recvtag, // с каким тегом должны получить
	MPI_Comm comm, // идентификатор группы
	MPI_Status *status // сколько реально данных получено
			    	   // можно получить тэг (MPI_ANY_TAG)
			    	   // src (если MPI_ANY_SOURCE)
```

Если хотим сделать попарный обмен _send_ <-> _dest_? Но тогда MPI приложение сразу убьют, потому что происходит отправление и получение асинхронно. 
Одинаковые данные на вход == аварийное завершение. Но если очень хочется, то можно!
```cpp
void MPI_Sendrecv_replace(
	void *buf, // что отправляем
	int count, // сколько
	MPI_Datatype type, // какого типа
	int dest, // кому
	int sendtag, // тэг отправляемого
	int src, // от кого
	int recvtag, // с каким тегом должны получить
	MPI_Comm comm, // идентификатор группы
	MPI_Status *status // сколько реально данных получено
			    	   // можно получить тэг (MPI_ANY_TAG)
			    	   // src (если MPI_ANY_SOURCE)

)
```

# Практикум по специальности
## Методы нахождения собственных значений
$Ax_{*}=\lambda x_{*}, ||x||= 1$
$Ax_{*} - \lambda x_{*}$ != 0 для компьютера
$Ax_{*} - \lambda x_{*} < \epsilon$ для компьютера
Но человечество такую задачу решать не умеет. Рассмотрим матрицу 7х7. 

![[IMG_8885.jpg]]
#### Матрица Фробениуса
$P(x) = sum(i=0, i=n-1, a_i * x^i)$
Ax=b - метод Гаусса
Если предположить, что существует такой алгоритм, что он будет находить собственные значения A, то мы сможем найти все решения многочлена степени n - 1.
При n >= 5 P_n(k) не может быть формулы
${A_k}$, $A_k$ подобна A, $A_k$ сходятся к верхнетреуголльной. Для лучших алгоритмов 2-3 итерации на каждое собственное значение. То есть (2-3)n с. з.
При переходе $A_k -> A_{k + 1}$ количество действий порядка n^3
#### 1 Упрощение
Приведём к почти треугольной матрице. Она будет унитарным подобием, то есть длины будут сохранены. Для этого используется либо метод вращений, либо отражений.
![[IMG_8886.jpg]]
![[IMG_8887.jpg]]
![[IMG_8890.jpg]]
![[IMG_8891 (2).jpg]]
![[IMG_8892.jpg]]
Для симметрических используются:
- Метод Якоби
- Метод Бисекций
Часто используется $|\lambda|$ <= $||A||$
![[IMG_8894.jpg]]
Есть методы, инвариантные на шаге k: $A_{k + 1} = A_k$

LR разложение иногда практически также сходится
$A_k = L_k * R_k$
$A_{k+1} = R_k * L_k$
Для 3хдиагональных матриц сложность n. 

Метод Холецкого
$A_k = R_k^t D R_k$
$A_{k+1} = R_k D R_k^t$

### Проверка
1) След инвариантен. Можем построить невязку. ПОЛЕЗНО ДЛЯ ОТЛАДКИ
$|trA - \sum\lambda_i| / ||A|| < \epsilon$
2) Сохраняется длина матрицы как вектора $n^2$
	$||x||= x^{*} * x$
	$||A||_{*} = \sqrt{\sum(a_{ij}*a_{ji})}$
	$|||A||_* - \sqrt{\sum(\lambda_i)}|/||A|| < \epsilon$

### Ускорение сходимости
Можем ли мы помочь какому-то диагональному элементу быстрее сходиться к собственному значению?
Будем строить разложение для $$B_k = A_k - Es = Q_kR_k$$
s - сдвиг.
$$\lambda_i^{(B)} = \lambda_i^{(A)} - s$$
$$A_{k+1} = R_kQ_k + Es_k$$
$$s \approx \lambda_n$$
1) s = a_{nn} - инвариант
2) s = a_{nn} +- 1/2a_{nn-1} почти нет
3) s - решение лин системы

## Специфика
QR разложение. Могло сойтись где-то в середине, тогда нужна проверка, чтобы матрица не поворачивалась на 0.

Также могло случиться, что на диагонали получился отрицательный элемент на почти диагонали.

#### LR и Холецкий
Существует не всегда
$$A - sE = LR$$
Если случайно угадали s, то работать не будет. Просто его запоминаем и работаем с подматрицей. Можно двигать s, чтобы получилось

# Практикум по специальности
Как работать с матрицей?
![[IMG_8899.jpg]]
Можно выделять доп память внутри каждого потока.