## Асинхронные обмены
Можно сделать обмен асинхронным даже для больших данных. Вы обещаете отправить кусок данных и не лезть туда, пока идёт отправка. 
Сама структура включает в себя плату со своим процессором и памятью:

![[IMG_8931.jpg]]

DMA - Direct Memory Access

```cpp
// Immediate - создание запроса на отправку
int MPI_Isend( // Отправка
	int *buf,
	int count,
	MPI_Datatype type,
	int dest,
	int tag,
	MPI_Comm comm,
	MPI_Request *request // Полный аналог FILE *
);


int MPI_Irecv( // Получение
	int *buf,
	int count,
	MPI_Datatype type,
	int src,
	int tag,
	MPI_Comm comm,
	MPI_Request *request
);
// MPI_status отсутствует, потому что это просто регистрация запроса


int MPI_test(
	MPI_Request *request, // сам запрос
	int *flag, // != 0, если завершён
	MPI_status *status, // если recv, то статус завершённого обмена
);

// Если произошло несколько обменов, то делаем
int MPI_test_any(
	int count, //число запросов
	MPI_Request a[],
	int *index, // номер завершения
	int *flag, // флаг завершения
	MPI_Status *status // Если recv
)

// Ожидание завершения запроса
int MPI_Wait(
	MPI_Request *request, // передаём идентификатор запроса
	MPI_status *status // для recv
)

// Ожидание завершения хотя бы одного запроса
int MPI_Wait_any(
	int count,
	MPI_Request request[], // передаём идентификатор запроса
	int *index, // номер завершившегося
	MPI_status *status // для recv
)
```

```c
MPI_Isend + MPI_Wait = MPI_Send
MPI_Irecv + MPI_Wait = MPI_Recv
Но также мы можем между действиями вставить ещё действия БЕСПЛАТНО
```

## Коллективные обмены
Есть разные варианты:
Один -> всем
Все -> одному (опасно)
Все -> всем

```cpp
int MPI_Abort( // пример один -> всем
	MPI_Comm comm,
	int error_code
); // Ей не пользуемся! Вызов в одной вызываетс коллабс всех процессов

int MPI_Barrier(
	MPI_Comm comm
); // Единственное разумное применение - измерение времени! 
   // Любое другое использование - ошибка!
```

Само измерение времени:
```cpp
MPI_Barrier(...)
t = get_time()
...
MPI_Barrier(...)
t = get_time() - t
```

```cpp
int MPI_Bcast( //broadcast - один -> всем
	void *buf, // для отправителя этот массив отправляется, для остальных перезаписываеся
	int count,
	MPI_Datatype type,
	int root, // от кого, номер процесса с помощью MPI_Comm_rank
	MPI_Comm comm // идентификатор группы
);
// Все реализации ТРЕБУЮТ, чтобы Bcast вызывался во всех процессорах с одинаковыми аргументами
```

#### Все -> одному
```cpp
int MPI_Reduce(
	void *send_buf, // У всех, даже у текущего процесса есть send_buf
	void *recv_buf,
	int count,
	MPI_Datatype type,
	MPI_Op op, // идентификатор операции, например MPI_SUM
	int root, // кому
	MPI_Comm comm
);

// Если требуется Все всем и сделать операцию, 
// то это делается, например, MPI_Reduce + MPI_Bcast
// или, что ещё лучше, за o(ln{p})
int MPI_Allreduce(
	void *send_buf,
	void *recv_buf,
	int count,
	MPI_Datatype type,
	MPI_Op op, // идентификатор операции, например MPI_SUM
	MPI_Comm comm
);
```

##### Сами идентификаторы операции, покомпонентные для вектора

| name       | description        |
| ---------- | ------------------ |
| MPI_SUM    | сумма              |
| MPI_MAX    | максимум           |
| MPI_MIN    | минимум            |
| MPI_PROD   | умножение          |
| MPI_LAND   | logical and - &&   |
| MPI_BAND   | & побитовое и      |
| MPI_LOR    | logical or - \|\|  |
| MPI_BOR    | побитовое или - \| |
| MPI_LXOR   | logical xor        |
| MPI_BXOR   | побитовое xor      |
| MPI_MAXLOC | MPI_DOUBLE_INT     |
| MPI_MINLOC | MPI_DOUBLE_INT     |
|            |                    |

```cpp
MPI_DOUBLE_INT
struct {
	double x;
	int y
}; // выбирается минимальный
```

В процесс с номером k положим `x = (мин норма обратной матрицы в процессе k) ^ -1, если есть и 0, если нет`, `у = номер строки в глобальной нумерации`
MAXLOC - максимум(мин норма обратной)$^{-1}$
Если нет, то не участвует

Глобальная нумерация
$a_{ij}$ <=> $a + i*n + j$
В MPI хранятся только свои строки, поэтому есть локальная нумерация

![[IMG_8933.jpg]]

##### Создание своих операций
```cpp
int MPI_Op_create(
	MPI_Uset_function *function,
	int commute, // 0, если не коммутирует, 1 - иначе
	MPI_Op *op // идентификатор созданной операции
); // ВЫДЕЛЯЕТ ПАМЯТЬ

typedef void(MPI_Userfunction)
(
	void *a,
	void *b,
	int *len,
	MPI_Datatype type
);

for (i = 0; i < *len; i++) {
	b[i] = a[i] op b[i];
}

void sum(void *a, void *b, int *len, MPI_Datatype type) {
	switch (type)
	{
		case MPI_DOUBLE:
			double *pa = (double*) a;
			double *pb = (double*) b;
			for (i = 0; i < *len; i++) {
				pb[i] += pb[i]; 
			}
		case MPI_INT:
			int *pa = (int*) a;
			int *pb = (int*) b;
			for (i = 0; i < *len; i++) {
				pb[i] += pb[i]; 
			}
	}
}
```
Её нужно в каждом из процессов вызывать

MPI_Op_create выделяет память в каждом процессе. Вызываем её в каждом процессе

### ВРЕМЯ в MPI программе
Для потока pthread

| T1  | T2  | T3  |
| --- | --- | --- |
|     |     |     |

После barrier поток переходит в режим ожидания и не тратит ресурсы.
CPU time = $t_{cpu}$
wall clock = $t_{total}$
$t_{cpu} <= t_{total}$ 
$\sum t_{cpu} => t_{total}$

MPI
1) Должно быть максимально быстро
2) Монопольное владение ресурсами CPU, ...

MPI использует для ожидания polling (непрерывный опрос готовности данных)
В MPI $t_{cpu} = t_{total}$

- 100% cpu
- $t_{cpu}$ у всех одинаковое

### Доп функции коллективного обмена

![[Pasted image 20241207142839.png]]

```cpp
int MPI_Scatter( // режет матрицу и отправляет всем
	void *sendbuf,
	int sendcount,
	MPI_Datatype sendtype,
	void *recvbuf,
	int recvcount,
	MPI_Datatype recvtype,
	int root,
	MPI_comm comm
)

MPI_Gather

MPI_Allgather // нет root
```